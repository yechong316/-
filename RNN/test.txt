Recurrent neural networks (RNNs) are very powerful dynamical systems and they are the natural way of using neural networks to map an input sequence to an output sequence, as in speech recognition and machine translation, or to predict the next term in a sequence, as in language modeling. However, training RNNs by using back-propagation through time to compute error-derivatives can be difﬁcult. Early attempts suffered from vanishing and exploding gradients and this meant that they had great difﬁculty learning long-term dependencies. Many different methods have been proposed for overcoming this difﬁculty.
A method that has produced some impressive results  is to abandon stochastic gradient descent in favor of a much more sophisticated Hessian-Free (HF) optimization method. HF operates on large mini-batches and is able to detect promising directions in the weight-space that have very small gradientsbut even smaller curvature. Subsequentwork, however,suggested that similar results could be achieved by using stochastic gradient descent with momentum provided the weights were initialized carefullyand large gradients were clipped . Further developments of the HF approach look promising  but are much harder to implement than popular simple methods such as stochastic gradient descent with momentum or adaptive learning rates for each weight that depend on the history of its gradients .
The most successful technique to date is the Long Short Term Memory (LSTM) Recurrent Neural Network which uses stochastic gradient descent, but changes the hidden units in such a way that the backpropagated gradients are much better behaved . LSTM replaces logistic or tanh hidden units with “memory cells” that can store an analog value. Each memory cell has its own input and output gates that control when inputs are allowed to add to the stored analog value and when this value is allowed to inﬂuence the output. These gates are logistic units with their own learned weights on connections coming from the input and also the memory cells at the previous time-step. There is also a forget gate with learned weights that controls the rate at which the analog value stored in the memory cell decays. For periods when the input and output gates are off and the forget gate is not causing decay, a memory cell simply holds its value over time so the gradient of the error w.r.t. its stored value stays constant when backpropagated over those periods.
